# Logging Best Practices

When building applications that write logs such as event logs or collected telemetry to Azure Storage blobs, there are a number of considerations. Not only is the number of write transactions potentially higher than that of an application that stores files such as media, but you must also consider features such as immutability and WORM which can impact how you configure your storage account(s).

## Key considerations

- How often do you flush/send data to storage?
- How many transactions are generated by a flush event? One? One hundred? One million? Do you batch multiple logged events in a single record/request or do you send each event as its own payload?
- Transactions are cost component of storage and you need to consider limits (hard and soft) of the platform as well.
- How will rights occur? As a single atomic unit, as a series of blocks, or appends?
- How will you authenticate to storage?

### Every write transaction is a charged event

With every transaction to Azure Storage blobs being a billable (or charged) event in addition to raw storage costs (*e.g* consumption), things can add up quickly if you are flushing logs from your collection service to Azure on a frequent basis. For example, if your logger is collecting data from a single node at a rate of once per second and you have 5,000 nodes all collecting data at the same time, you'll have:

- 300,000 write transactions per minute
- 18,000,000 write transactions per hour
- 432,000,000 write transactions per day

> Note: A write transaction in this context is a Put Blob or Put Block (plus Put Block List) request to the Azure Storage REST API.

Cost of those transactions may influence your design as costs for write operations vary between storage tiers such as Hot and Premium. Lets use the current pricing for Azure blobs in Azure's West US 2 region for our example. Current pricing for write operations (per 10,000) in this region is $0.05 for the Hot tier and $0.0175 for Premium (see [Block blob pricing](https://azure.microsoft.com/pricing/details/storage/blobs/) for the most current pricing).

Using the number of transactions calculated previously, you can baseline cost across each tier for transactions.

| Transactions (raw) | Region    | Performance tier | Cost per 10k | Cost (per minute) | Cost (per hour) | Cost (per day) |
| ------------------ | --------- | ---------------- | ------------ | ----------------- | --------------- | -------------- |
| 432,000            | West US 2 | Hot              | $0.05        | $1.50             | $90.00          | $2,160.00      |
| 432,000            | West US 2 | Premium          | $0.0175      | $0.525            | $31.50          | $756           |

If the data is short-lived in Azure Storage - meaning you are using blobs as an intermediary layer or part of a broader data movement and analysis solution and as soon as data has been picked up by the next stage of your pipeline you delete it â€“ you might consider writing directly to the Premium tier. While the price per GiB is higher for storage costs, the transaction prices are lower as demonstrated above - on the order of 65% less per day in the target region assuming consistent throughput. Azure also promises higher throughput and lower latency in the Premium tier.

### Frequent flushes and transaction limits

Frequent flushes of the source buffer can also cause you to hit the transaction limit for your target storage account. By default, all blob storage accounts have a maximum of 20,000 transactions per second (see [Scale targets for standard storage accounts
](https://docs.microsoft.com/azure/storage/common/scalability-targets-standard-account#scale-targets-for-standard-storage-accounts)). This is a soft limit and can be adjusted based on your scenario and the availability of capacity in region to raise the limit, but it is best to design your solution to stay within this limit.

### Put Blob versus Put Block - Which one is right for me?

The [Put Blob](https://docs.microsoft.com/rest/api/storageservices/put-blob) API allows you to commit an entire object, up to 256MiB, with one atomic API call. This is the simplest approach, assuming you want to place one file per PUT. If instead, you want to stream the data and eventually finalize the commit of a single file, you can use [Put Block](https://docs.microsoft.com/rest/api/storageservices/put-block) in combination with [Put Block List](https://docs.microsoft.com/rest/api/storageservices/put-block-list) to write larger files or write your files in smaller chunks.

### The API sounds complex. Don't you have an SDK?

The storage client libraries (SDK) abstract away a lot of the API complexity for you at the expense of hiding API level decisions like which path to take for operations like Put Blob. For logging scenarios, it is typically recommended that you invoke the REST API directly as it will give you the most granular control, however if a storage client library is available in the same language you use for your logger, you may want to explorer if the client libraries will allow you the controls you need with the performance you require.

Storage client libraries are available for multiple languages, including [.NET](https://docs.microsoft.com/dotnet/api/overview/azure/storage?view=azure-dotnet), [Java](https://docs.microsoft.com/java/api/overview/azure/storage), [Node.js](https://azure.github.io/azure-storage-node), [Python](https://azure-storage.readthedocs.io/), [PHP](https://azure.github.io/azure-storage-php/), and [Ruby](https://azure.github.io/azure-storage-ruby).

### Authentication to your storage account

How you choose to authenticate to your storage accounts is highly dependent on where your solution resides. For example, does the application sending logs to storage reside in Azure or on a client device?

If the solution that is generating the logs resides outside of Azure, it is generally recommended that you implement Shared Access Signatures (SAS) in combination with SAS Policy. (see [Best practices when using SAS](https://docs.microsoft.com/azure/storage/common/storage-sas-overview#best-practices-when-using-sas). A service SAS can be created through the REST API (see [Create a service SAS](https://docs.microsoft.com/rest/api/storageservices/create-service-sas)) along with a stored access policy (see [Define a stored access policy](https://docs.microsoft.com/rest/api/storageservices/define-stored-access-policy)). The creation of a policy is performed through the [Set Container ACL](https://docs.microsoft.com/rest/api/storageservices/set-container-acl) API.

Shared access signatures can be scoped to very granular permissions. A collection of examples can be found at [Service SAS examples](https://docs.microsoft.com/rest/api/storageservices/service-sas-examples) and an example of executing a Put Blob operation with a service SAS through Bash/curl at [generatesasanduploadblob.sh](https://github.com/ciphertxt/LearningAzureStorageREST/blob/master/generatesasanduploadblob.sh)

If your solution is running in Azure, you can also use SAS and SAS Policy, but this can now be combined with [Managed identities](https://docs.microsoft.com/azure/active-directory/managed-identities-azure-resources/overview). This allows you to use a service principal to authenticate with Azure storage and perform operations (see [Authorize access to blob and queue data with managed identities for Azure resources](https://docs.microsoft.com/azure/storage/common/storage-auth-aad-msi)).
